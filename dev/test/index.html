<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Testing · ParlinfoSpeechScraper Documentation</title><meta name="title" content="Testing · ParlinfoSpeechScraper Documentation"/><meta property="og:title" content="Testing · ParlinfoSpeechScraper Documentation"/><meta property="twitter:title" content="Testing · ParlinfoSpeechScraper Documentation"/><meta name="description" content="Documentation for ParlinfoSpeechScraper Documentation."/><meta property="og:description" content="Documentation for ParlinfoSpeechScraper Documentation."/><meta property="twitter:description" content="Documentation for ParlinfoSpeechScraper Documentation."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">ParlinfoSpeechScraper Documentation</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">ParlinfoSpeechScraper</a></li><li><a class="tocitem" href="../download/">XML download</a></li><li><a class="tocitem" href="../sgml2xml/">XML download (1981-1997)</a></li><li><a class="tocitem" href="../usage/">Usage</a></li><li><a class="tocitem" href="../advusage/">Advanced Usage</a></li><li><a class="tocitem" href="../nodes/">Nodes</a></li><li><a class="tocitem" href="../functionreference/">Function references</a></li><li class="is-active"><a class="tocitem" href>Testing</a><ul class="internal"><li><a class="tocitem" href="#How-to-test"><span>How to test</span></a></li><li><a class="tocitem" href="#Gold-standard-testing"><span>Gold standard testing</span></a></li><li><a class="tocitem" href="#Dates-summary-test"><span>Dates summary test</span></a></li><li><a class="tocitem" href="#Input-test-toml-file"><span>Input test toml file</span></a></li></ul></li><li><a class="tocitem" href="../common_errors/">Common errors and warnings</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Testing</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Testing</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/Australian-Parliamentary-Speech/Scraper" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/Australian-Parliamentary-Speech/Scraper/blob/main/docs/src/test.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Testing"><a class="docs-heading-anchor" href="#Testing">Testing</a><a id="Testing-1"></a><a class="docs-heading-anchor-permalink" href="#Testing" title="Permalink"></a></h1><h2 id="How-to-test"><a class="docs-heading-anchor" href="#How-to-test">How to test</a><a id="How-to-test-1"></a><a class="docs-heading-anchor-permalink" href="#How-to-test" title="Permalink"></a></h2><p>Head into Scraper directory after cloning, in terminal type: <code>julia</code></p><p>In Julia REPL, press <code>]</code></p><p>Then run test by <code>test</code></p><p>The suite of tests will be run. There are three sets of tests you can run, gold standard testing, dates summary and toy xml testing. </p><h2 id="Gold-standard-testing"><a class="docs-heading-anchor" href="#Gold-standard-testing">Gold standard testing</a><a id="Gold-standard-testing-1"></a><a class="docs-heading-anchor-permalink" href="#Gold-standard-testing" title="Permalink"></a></h2><p>To ensure the quality of our data extraction process—from XML files to CSV files before database upload—we use a <strong>gold-standard testing approach</strong>. This helps us measure how accurately our scraper reproduces the information contained in Hansard documents.</p><p><strong>Gold-standard CSV files</strong> refer to manually corrected CSV files. These files are produced by research assistants who compare:</p><ul><li>the scraped CSV output  </li><li>the original XML  </li><li>and the corresponding PDF version of Hansard</li></ul><p>The gold-standard files correct only <strong>computationally feasible errors</strong>, such as mis-split speeches or missing flags, but they do <strong>not</strong> fix issues that are impossible to correct automatically (e.g., typos in the original XML).</p><p>Each pair of files—<strong>gold standard vs scraped output</strong>—is evaluated using a <strong>similarity score</strong>, which measures how closely our output matches the gold standard.</p><h3 id="How-the-Similarity-Score-Works"><a class="docs-heading-anchor" href="#How-the-Similarity-Score-Works">How the Similarity Score Works</a><a id="How-the-Similarity-Score-Works-1"></a><a class="docs-heading-anchor-permalink" href="#How-the-Similarity-Score-Works" title="Permalink"></a></h3><p>There are two key challenges when comparing two CSV files:</p><ol><li><strong>Row counts may differ</strong>, because speeches may be merged or split.</li><li><strong>The same row may not appear in the same position</strong>, so we need a reliable way to match rows.</li></ol><p>To address this, we use the <strong>speech content as the row identifier</strong>, since it is typically the most unique attribute.</p><h4 id="Two-Matching-Modes"><a class="docs-heading-anchor" href="#Two-Matching-Modes">Two Matching Modes</a><a id="Two-Matching-Modes-1"></a><a class="docs-heading-anchor-permalink" href="#Two-Matching-Modes" title="Permalink"></a></h4><p>We support two matching modes:</p><h5 id="1.-Exact-Mode"><a class="docs-heading-anchor" href="#1.-Exact-Mode">1. Exact Mode</a><a id="1.-Exact-Mode-1"></a><a class="docs-heading-anchor-permalink" href="#1.-Exact-Mode" title="Permalink"></a></h5><p>In <strong>exact mode</strong>, the full speech text from the gold standard is used to find an identical match in the scraped output.</p><p>Limitation: even a tiny difference—like a missing space or a changed quotation mark—can prevent a match.This can cause the score to appear lower than it should be.</p><h5 id="2.-Fuzzy-Mode"><a class="docs-heading-anchor" href="#2.-Fuzzy-Mode">2. Fuzzy Mode</a><a id="2.-Fuzzy-Mode-1"></a><a class="docs-heading-anchor-permalink" href="#2.-Fuzzy-Mode" title="Permalink"></a></h5><p>To reduce false mismatches, <strong>fuzzy mode</strong> uses <strong>partial text sampling</strong> instead of the full speech.</p><p><strong>How it works:</strong></p><ul><li>Extract short samples from the gold standard speech.</li><li>Each sample is a set of consecutive words.</li><li>Search for these samples in the scraped output.</li><li>If multiple candidate rows match the samples, select the one that is overall most similar to the gold standard speech.</li></ul><p>This allows us to match rows even when minor differences exist.</p><h4 id="Scoring"><a class="docs-heading-anchor" href="#Scoring">Scoring</a><a id="Scoring-1"></a><a class="docs-heading-anchor-permalink" href="#Scoring" title="Permalink"></a></h4><p>The test system implemented in <code>test/runtests.jl</code> provides a comprehensive framework for comparing generated CSV outputs against gold-standard CSV files and computing similarity ratios.</p><p>The system compares generated CSV files with gold-standard files stored in <code>test/gold_standard/</code>. For each test run, sample CSV files are first copied from the main output directory into <code>test/sample_csv/</code>. The similarity between each pair of files is then computed using the <code>similarity_csv</code> function defined in <code>similarity_funcs.jl</code>.</p><p>The comparison process works by first matching rows between the gold-standard file and the sample file based on speech content. Once a row is matched, the similarity score for that row is calculated as:</p><p>Row similarity = (number of matching cells) / (total number of cells)</p><p>where all remaining columns are compared, except for those explicitly excluded via the <code>skip_cols</code> parameter. This allows users to ignore metadata fields that are not relevant for assessing scrape quality.</p><p>The system returns <strong>two similarity ratios</strong>:</p><ul><li><p><strong>Similarity ratio</strong>   The similarity ratio between the generated sample CSV file and the corresponding gold-standard CSV file.</p></li><li><p><strong>Maximum similarity ratio</strong>   The similarity ratio obtained by comparing the gold-standard CSV file with itself.</p></li></ul><p>The maximum similarity ratio is generally <strong>less than 1</strong>, even though the files being compared are identical. This is expected behaviour and reflects inherent limitations of content-based row matching. In particular, some speeches consist of very short or highly repetitive phrases (such as <em>“hear, hear”</em>), which are not unique identifiers. In such cases, the matching algorithm cannot always determine unambiguously which row corresponds to which, leading to a non-perfect similarity score even for identical files.</p><h2 id="Dates-summary-test"><a class="docs-heading-anchor" href="#Dates-summary-test">Dates summary test</a><a id="Dates-summary-test-1"></a><a class="docs-heading-anchor-permalink" href="#Dates-summary-test" title="Permalink"></a></h2><p>This summary test checks for missing or unprocessed dates in the scraping pipeline.   It runs only when <code>&quot;summary&quot;</code> is included in <code>which_tests</code>.</p><p>The test performs the following checks:</p><ul><li><strong>XML vs CSV coverage</strong>   Identifies dates that appear in the XML input but do not have a corresponding CSV output, indicating XML files that were not processed.</li></ul><ul><li><strong>XML vs official sitting days</strong>   Compares XML dates against an authoritative list of parliamentary sitting days to identify sitting days for which no XML file exists.</li></ul><p>Depending on <code>which_house</code>, the comparison is performed against either the House of Representatives or Senate sitting calendar.</p><p>For diagnostic purposes, the test writes lists of problematic dates to:</p><ul><li><code>test_outputs/dates/only_in_xml_&lt;house&gt;.csv</code></li><li><code>test_outputs/dates/only_in_sitting_&lt;house&gt;.csv</code></li></ul><p>The test always passes and is intended to provide a diagnostic summary rather than enforce a hard failure.</p><h3 id="Toy-XML-Tests-(Edge-Case-XML-Testing)"><a class="docs-heading-anchor" href="#Toy-XML-Tests-(Edge-Case-XML-Testing)">Toy XML Tests (Edge-Case XML Testing)</a><a id="Toy-XML-Tests-(Edge-Case-XML-Testing)-1"></a><a class="docs-heading-anchor-permalink" href="#Toy-XML-Tests-(Edge-Case-XML-Testing)" title="Permalink"></a></h3><p>This test block implements <strong>toy XML tests</strong>, which are designed to validate the XML parsing pipeline using small, hand-crafted XML files that target specific edge cases.</p><p>The test runs only when <code>&quot;toy_xml_test&quot;</code> is included in <code>which_tests</code>.</p><h4 id="Purpose"><a class="docs-heading-anchor" href="#Purpose">Purpose</a><a id="Purpose-1"></a><a class="docs-heading-anchor-permalink" href="#Purpose" title="Permalink"></a></h4><p>The goal of these tests is to ensure correct behaviour on edge cases that are:</p><ul><li>small and easy to inspect,</li><li>quick to run,</li><li>and simple for users to extend.</li></ul><p>By working with minimal XML examples, it becomes straightforward to verify whether specific XML structures are handled correctly.</p><h4 id="How-the-Test-Works"><a class="docs-heading-anchor" href="#How-the-Test-Works">How the Test Works</a><a id="How-the-Test-Works-1"></a><a class="docs-heading-anchor-permalink" href="#How-the-Test-Works" title="Permalink"></a></h4><ul><li><p>The test iterates over multiple XML parsing phases:</p><ul><li><code>AbstractPhase</code></li><li><code>Phase2011</code></li><li><code>PhaseSGML</code></li></ul></li><li><p>For each phase:</p><ul><li>XML test files are read from <code>test/xmls/&lt;Phase&gt;/</code>.</li><li>Each XML file is processed using the standard XML-to-CSV pipeline.</li><li>Intermediate files are cleaned according to the test configuration.</li><li>The resulting CSV output is renamed to a deterministic filename.</li></ul></li><li><p>The generated CSV files are then compared against gold-standard CSVs stored in:</p><ul><li><code>xml_gold_standard/&lt;Phase&gt;/</code></li></ul></li><li><p>Each comparison reports whether the generated CSV matches the gold standard.</p></li></ul><p>The test always returns <code>true</code>; its role is to provide validation feedback rather than enforce a hard failure.</p><h4 id="Adding-a-New-Edge-Case"><a class="docs-heading-anchor" href="#Adding-a-New-Edge-Case">Adding a New Edge Case</a><a id="Adding-a-New-Edge-Case-1"></a><a class="docs-heading-anchor-permalink" href="#Adding-a-New-Edge-Case" title="Permalink"></a></h4><p>Adding a new XML edge case is intentionally simple. Users only need to insert two lines into Scraper when generating the test XML:</p><pre><code class="language-julia hljs">edge_case = &quot;PNode_name_in_pnode&quot;
write_test_xml(node, parent_node, edge_case)</code></pre><h2 id="Input-test-toml-file"><a class="docs-heading-anchor" href="#Input-test-toml-file">Input test toml file</a><a id="Input-test-toml-file-1"></a><a class="docs-heading-anchor-permalink" href="#Input-test-toml-file" title="Permalink"></a></h2><p>This testing requires two toml files, one is the same input file for the Scraper program (with slight modification of the output_path), and the other one specifically designed for this testing suite. </p><h3 id="Test-Parameters"><a class="docs-heading-anchor" href="#Test-Parameters">Test Parameters</a><a id="Test-Parameters-1"></a><a class="docs-heading-anchor-permalink" href="#Test-Parameters" title="Permalink"></a></h3><ul><li><p><strong><code>skip_cols</code></strong>   A list of column names to exclude from the similarity comparison.  </p></li><li><p><strong><code>which_test</code></strong>   Specifies the matching strategy used to align rows between the gold-standard CSV and the scraped output.  </p><ul><li><code>&quot;exact&quot;</code>: Rows are matched only if the full speech text matches exactly.  </li><li><code>&quot;fuzzy&quot;</code>: Rows are matched using sampled word sequences from the speech text, allowing for minor textual differences.</li></ul></li><li><p><strong><code>fuzzy_search</code></strong>   Controls how speech text is sampled in fuzzy matching mode.   This option is only used when <code>which_test = &quot;fuzzy&quot;</code>.   The value is a two-element array:</p><ul><li>The first element specifies the number of consecutive words in each sample.</li><li>The second element specifies the step size (interval) between successive samples.  </li></ul><p>For example, <code>[5, 2]</code> means that samples of 5 consecutive words are taken, starting every 2 words along the speech.</p></li><li><p><strong><code>which_house</code></strong>   Indicates which parliamentary house the data belongs to and should be tested against.   This is used to select the appropriate gold-standard reference files and parsing rules.   Common values include:</p><ul><li><code>&quot;house&quot;</code>: House of Representatives  </li><li><code>&quot;senate&quot;</code>: Senate</li></ul></li></ul><h3 id="Sample-input-file"><a class="docs-heading-anchor" href="#Sample-input-file">Sample input file</a><a id="Sample-input-file-1"></a><a class="docs-heading-anchor-permalink" href="#Sample-input-file" title="Permalink"></a></h3><pre><code class="nohighlight hljs">[ test_params ]
    skip_cols = [&quot;speaker_no&quot;,&quot;non_speech_flag&quot;,&quot;page.no&quot;,&quot;name&quot;,&quot;electorate&quot;,&quot;party&quot;,&quot;role&quot;,&quot;path&quot;,&quot;Speaker&quot;,&quot;Time&quot;,&quot;Other&quot;]
    #or exact
    which_test = &quot;fuzzy&quot;
    fuzzy_search = [5,2]
    which_house = &quot;house&quot;
    which_tests = [&quot;gold_standard&quot;,&quot;summary&quot;,&quot;toy_xml_test&quot;,&quot;MP_specific_gs&quot;]</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../functionreference/">« Function references</a><a class="docs-footer-nextpage" href="../common_errors/">Common errors and warnings »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Tuesday 20 January 2026 05:12">Tuesday 20 January 2026</span>. Using Julia version 1.12.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
